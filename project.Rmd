---
title: "STATT253 Group Project"
author: "Ikran, Colleen, Sebastian, Amritha"
date: "4/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r, echo=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data
library(moderndive) #for data
library(rattle) #weather data
library(cluster)

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests

theme_set(theme_minimal())
```

```{r, include=FALSE}
attrition <- read.csv("attrition.csv", fileEncoding="UTF-8-BOM")

#check if we have missing values..
# attrition %>%
#   add_n_miss() %>%
#   arrange(desc(n_miss_all))

attrition <- attrition %>% 
mutate(WorkLifeBalance = fct_recode(as.factor(WorkLifeBalance), Bad = "1", Good = "2", Better = "3", Best = "4"),
       EnvironmentSatisfaction = fct_recode(as.factor(EnvironmentSatisfaction), Low="1", Medium = "2", High = "3", `Very High` = "4"))

#want to remove -0.04 from YearsAtCompany
# attrition %>%
#   arrange(YearsAtCompany)

#split into training and testing
set.seed(253)
attrition_split <- initial_split(attrition, prop = .7)

attrition_train <- training(attrition_split)
attrition_test <- testing(attrition_split)
```


>*Employee Attrition: Why do people leave their jobs?*

Many people remain at their jobs for a long time, but some people inevitably end up quitting.  Wouldn't it be beneficial to be able to tell which potential (or current) employees have a higher likelihood of leaving the job, and which are likely to stay?

This knowledge could be used by the employers for good.  For example, they could see what factors influence employee attrition that can be changed to better the employee's experience, such as whether or not the employee works overtime.  It could also be used for not so good reasons, if a model predicts that a potential employee is likely to quit, and that leads to them not even being considered for the job.

We wanted to explore this more, to see if we could accurately predict whether or not an employee will leave their job.  We used Kaggle's (fictional) attrition dataset, which contains data from 1470 employees. In the original dataset, about 83.9% of employees were replaced, while 16.1% resulted in attrition. These percentages are shown in the plot below.

```{r}
ggplot(data = attrition, aes(x=Attrition)) +
  geom_bar(color = 'blue', fill = 'lightblue') +
  ggtitle("Employee Attrition in Full Dataset")
```


The dataset included 35 variables:

```{r, echo=FALSE}
names(attrition)
```

>Data Cleaning

To clean the data, we recoded the levels of the WorkLifeBalance and EnvironmentSatisfaction variables to be more meaningful to the viewer, rather than just easily-misinterperable numbers. Originally these variables were coded as numbers 1-4, but we refactored them to take on their original values, e.g. "High" or "Good".


>Our hypothesis

We then tried to brainstorm what variables (out of the 35 in the attrition data) might be important predictors of whether or not an employee will quit their job.  We split the data into training and testing, and created exploratory plots of some of the variables using the training dataset.  A few variables that we thought would be important were `OverTime`, `YearsAtCompany`, `Age`, `WorkLifeBalance`, and `EnvironmentSatisfaction`. The relationships of these variables to `Attrition` are visualized below.

```{r, echo=FALSE}
attrition_train %>%
  # mutate(age_range <- cut(attrition$Age, 6)) %>%
  ggplot(aes(fill = Attrition , x = cut(Age,4, labels=c("18-28", "29-39", "40-49", "50-60")))) +
  geom_bar(position = "fill") +
  labs(x = "Age range", y = "% Attrition")+
  scale_fill_brewer(palette = "Blues") 
```

```{r, echo=FALSE}
#overtime vs attrition
attrition %>%
  ggplot(aes(fill = Attrition, x = OverTime)) +
  geom_bar(position = "fill")+
  scale_fill_brewer(palette = "Greens")
```

```{r, echo=FALSE}
#work life balance vs attrition
attrition %>%
  ggplot(aes(fill = Attrition, x = WorkLifeBalance)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Reds")
```


```{r, include=FALSE}
attrition %>%
  ggplot(aes(x=YearsAtCompany)) +
  geom_density()
```

```{r, echo=FALSE}
#a different look at years at company and attrition
attrition %>%
  ggplot(aes(fill=Attrition, x=cut(YearsAtCompany, c(-1,5,10,20,41), labels=c("0-5","6-10", "11-20", "21+")))) +
  geom_bar(position = "fill") +
  labs(x="Years at company", y="% Attrition")
```

```{r, echo=FALSE}
# environment satisfactions vs attrition
attrition %>%
  ggplot(aes(fill = Attrition, x = EnvironmentSatisfaction)) +
  geom_bar(position = "fill")

#1 'Low'
#2 'Medium'
#3 'High'
#4 'Very High'
```

The first model we built was a logistic regression model using these variables (`OverTime`, `YearsAtCompany`, `Age`, `WorkLifeBalance`, and `EnvironmentSatisfaction`) that we initially suspected to have an influence on attrition rates.


*1st model*
```{r}
set.seed(253)

attrition_mod1 <- train(
    Attrition ~ OverTime + YearsAtCompany + Age + WorkLifeBalance + EnvironmentSatisfaction ,
    data = attrition_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r, echo=FALSE}
summary(attrition_mod1) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))

# Those who worked overtime have 4.31 times the odds of attriting than those who don't work overtime. 
```

We can interpret the exponentiated coefficients of this model as multipliers to the odds of an employee leaving resulting in attrition. For example, the exponentiated coefficient on the `OverTimeYes` variable of 4.308 means that employees who worked overtime have their odds of leaving resulting in attrition multiplied by 4.308 compared to employees who did not work overtime.


>Model Evaluation

To evaluate this model, we looked at the accuracy rate, as well as the sensitivity and specificty. This model had an accuracy rate of 84.9%. This appears good at first glance, but it is very close to the no information rate (the rate at which a correct guess can be made with no information) of the data, which is 83.6%. Additionally, we see that the sensitivity is 14.2% and the specificity is 98.7%. This means that the model is mostly guessing no attrition for every case and getting the actual no attrition cases right almost all the time and the actual attrition cases wrong almost all the time. We want our model to be able to detect attrition, so this isn't what we want in our model.

```{r, echo=FALSE}

confusionMatrix(data = predict(attrition_mod1, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

# Accuracy rate of 84.9%... BUT it isn't much better than the No Information Rate (predicting these by chance) of 83.6%...
# we care more about the sensitivity, which is the percentage of correct predictions that people would leave their job out of all of the people who did end up leaving their job
# sensitivity is .13; therefore, this model does not seem very good

# CV accuracy of 84.5%
attrition_mod1$results$Accuracy
```

Our second model uses Logistic Regression using all variables except Over18 and StandardHours. These two variables only have one factor, so they are meaningless in our model-building. 

*2nd Model*
```{r}


set.seed(253)

# Perform logistic regression
attrition_allvars <- train(
    Attrition ~ . ,
    data = attrition_train %>% select(-Over18, -StandardHours),
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

summary(attrition_allvars) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))
```


```{r}
confusionMatrix(data = predict(attrition_allvars, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 
# training Accuracy rate of 89% when the model fits all variables
# No information rate of 83.6%, so this model is slightly better
# sensitivity is a lot better than the previous model, at 50.9%

# CV accuracy of 87.5%
attrition_allvars$results$Accuracy
```

Now we made a variable importance plot to see what variables are most crucial to include in a model.
```{r}
vip(attrition_allvars$finalModel, num_features = 30, bar = FALSE)
```


Our third model uses logistic regression with the top 5 best variables as shown in the Importance plot above.

*3rd model*
```{r}
set.seed(253)


attrition_bestvars <- train(
    Attrition ~ OverTime + BusinessTravel + JobInvolvement + JobSatisfaction + EnvironmentSatisfaction  ,
    data = attrition_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
confusionMatrix(data = predict(attrition_bestvars, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

# training Accuracy of 83.87% when fit on the top 5 best models.  Practically the same as the No information rate though...
# sensitivity is 11.2%... pretty bad...

# CV accuracy is 84.5%
attrition_bestvars$results$Accuracy
```


For our 4th model, lets try a lasso model to see if it can penalize unimportant variables and get a better accuracy and sensitivity.

*4th model*
```{r}
set.seed(253)

lambda_grid <- 10^seq(-4, 0, length = 100)

attrition_lasso <- train(
    Attrition ~ .,
    data = attrition_train %>% select(-Over18, -StandardHours),
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 10^seq(-4, 0, length = 100)),
    metric = "Accuracy",
    na.action = na.omit
)


attrition_lasso$results %>% head()
attrition_lasso$bestTune$lambda 

```

#Potential best model
```{r}
#training accuracy
confusionMatrix(data = predict(attrition_lasso, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

#CV accuracy
attrition_lasso$results %>%
  filter(lambda == attrition_lasso$bestTune$lambda)

# training accuracy is 89.3% (an improvement from the NIR of 83.6%!)
# sensitivity is 47.9%

# CV accuracy is 88.2% with a lambda value of 0.001232847		
```



```{r}
#plot of lambda values versus accuracy
attrition_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line() +
  scale_x_log10() 


#not sure why this is here...
# attrition_lasso$resample %>% 
#   summarize(avg_accuracy = mean(Accuracy))
```

Now lets make another lasso model but with all variables, except Over18 and StandardHours, to see if this results in a difference accuracy.

*5th Model*
```{r}
attrition_lasso_best <- train(
    Attrition ~ .,
    data = attrition_train %>% select(-Over18, -StandardHours),
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 0.0005336699),
    metric = "Accuracy",
    na.action = na.omit
)

confusionMatrix(data = predict(attrition_lasso_best, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 
```

Now we can examine which coefficiants aren't helpful and also look at which coefficiants are the most important. Do you see a difference from the previous importance plot?
```{r}
coefficients(attrition_lasso$finalModel,0.0005336699)
#DepartmentSales, EmployeeCount, JobLevel, MonthlyIncome, and PerformanceRating all shrunk to 0.


#variable importance plot on lasso model with the best lambda
vip(attrition_lasso_best$finalModel, num_features = 30, bar = FALSE)

#interesting! different variables are the most important this time around... should we try a lasso model with only the top 6-10 important variables, because the others are kinda close to 0? What do you think?


```


Do we need another classification tree? Don't want to delete until we all agree -Ikran
```{r}
set.seed(253)

attrition_tree_2 <- train(
  Attrition ~ .,
  data = attrition_train %>% select(-Over18, -StandardHours),
  method = "rpart",
  tuneGrid = data.frame(cp = 0.025),
  trControl = trainControl(method = "cv", number = 5),
  metric = "Accuracy",
  na.action = na.omit
)

confusionMatrix(data = predict(attrition_tree_2, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

attrition_train %>%
  bind_cols(predict(attrition_tree_2, type = "prob")) %>%
  mutate(Attrition_numeric = ifelse(Attrition == "Yes",1,0)) %>%
  summarize(MSE = mean((Attrition_numeric-Yes)^2))
#average squared error between actual and predicted probabilities


attrition_train %>%
  bind_cols(predict(attrition_tree_2, type = "prob")) %>%
  ggplot(aes(x=Yes))+
  geom_histogram()
```

#MSE


Comments:

Should we make a table using all the results? This would reduce the code we have and we can use the kable function that she mentioned. I included all the valeus from the previous five models so we can make a nice table -- Ikran 



1st model:
Accuracy: 0.8484
Specificity: 0.98721
CV acc: 0.8416292


2nd model: 
Accuracy: 0.897
Specificity: 0.8785603
CV acc: 0.8785603

3rd model: 
Accuracy: 0.8397 
Specificity: 0.8416008
CV acc: 0.8416008


4th model: 
Accuracy:0.8931
Best tune lambda: 0.001232847
Best tune accuracy: 0.8824485


5th model: 
Accuracy: 0.896 
Specificity: 0.8416008
CV acc: 0.8416008


