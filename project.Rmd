---
title: "STATT253 Group Project"
author: "Ikran, Colleen, Sebastian, Amritha"
date: "4/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r, echo=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data
library(moderndive) #for data
library(rattle) #weather data
library(cluster)

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests

theme_set(theme_minimal())
```
1. -introduce data
-introduce research question
-include a graph that summarizes the response variable (i did this below)
-maybe talk about variables we thought might be important in predicting attrition? idk
-talk about data cleaning (hmm we only changed the number levels of WorkLifeBalance to Bad,Good,Better,Best right? We should probably do the same with environment satisfaction)



```{r, echo = FALSE}
attrition <- read.csv("attrition.csv", fileEncoding="UTF-8-BOM")
names(attrition)


#check if we have missing values..
# attrition %>%
#   add_n_miss() %>%
#   arrange(desc(n_miss_all))

attrition <- attrition %>% 
mutate(WorkLifeBalance = fct_recode(as.factor(WorkLifeBalance), Bad = "1", Good = "2", Better = "3", Best = "4"),
       EnvironmentSatisfaction = fct_recode(as.factor(EnvironmentSatisfaction), Low="1", Medium = "2", High = "3", `Very High` = "4"))

#want to remove -0.04 from YearsAtCompany
# attrition %>%
#   arrange(YearsAtCompany)

#split into training and testing
set.seed(253)
attrition_split <- initial_split(attrition, prop = .7)

attrition_train <- training(attrition_split)
attrition_test <- testing(attrition_split)
```
```{r}
#summarise response variable (proportions)
table(attrition_train$Attrition) %>%
  prop.table()

ggplot(data = attrition_train, aes(x=Attrition)) +
  geom_bar(color = 'blue', fill = 'lightblue') +
  ggtitle("Attrition in Training Dataset")

# From this plot we can see that in our training dataset, there are many more cases in which the employee stayed at their job than ones who left it.

# In the proportion table, we can see that 16.4% of the cases were ones who attrited, while 83.6% of the cases remained at their job.
```

## Potential research question: What factors may lead a potential employee to attrite?


```{r}
attrition_train %>%
  # mutate(age_range <- cut(attrition$Age, 6)) %>%
  ggplot(aes(fill = Attrition , x = cut(Age,4, labels=c("18-28", "29-39", "40-49", "50-60")))) +
  geom_bar(position = "fill") +
  labs(x = "Age range", y = "Proprotion of employees in age range")
```


```{r}
attrition %>%
  ggplot(aes(x=YearsAtCompany)) +
  geom_density()
```

```{r}
#a different look at years at company and attrition
attrition %>%
  ggplot(aes(fill=Attrition, x=cut(YearsAtCompany, c(-1,5,10,20,41), labels=c("0-5","6-10", "11-20", "21+")))) +
  geom_bar(position = "fill") +
  labs(x="Years at company", y="Proportion of employees in experience range")
```



```{r}
#work life balance vs attrition
attrition %>%
  ggplot(aes(fill = Attrition, x = WorkLifeBalance)) +
  geom_bar(position = "fill")

#WorkLifeBalance 
#1 'Bad'
#2 'Good'
#3 'Better'
#4 'Best'
```

```{r}
attrition %>%
  ggplot(aes(fill = Attrition, x = EnvironmentSatisfaction)) +
  geom_bar(position = "fill")

#1 'Low'
#2 'Medium'
#3 'High'
#4 'Very High'
```

```{r}
#overtime vs attrition
attrition %>%
  ggplot(aes(fill = Attrition, x = OverTime)) +
  geom_bar(position = "fill")

#potentially use in model
```



```{r}
# first we did a logistic regression model with the variables that we built exploratory plots of

set.seed(253)

attrition_mod1 <- train(
    Attrition ~ OverTime + YearsAtCompany + Age + WorkLifeBalance + EnvironmentSatisfaction ,
    data = attrition_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

summary(attrition_mod1) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))

# Those who worked overtime have 4.31 times the odds of attriting than those who don't work overtime. 
```

```{r}

confusionMatrix(data = predict(attrition_mod1, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

# Accuracy rate of 84.9%... BUT it isn't much better than the No Information Rate (predicting these by chance) of 83.6%...
# we care more about the sensitivity, which is the percentage of correct predictions that people would leave their job out of all of the people who did end up leaving their job
# sensitivity is .13; therefore, this model does not seem very good

# CV accuracy of 84.5%
attrition_mod1$results$Accuracy
```


```{r}
# Logistic Regression using all variables except Over18 and StandardHours. These two variables only have one factor, so they are meaningless in our model-building. 

set.seed(253)

# Perform logistic regression
attrition_allvars <- train(
    Attrition ~ . ,
    data = attrition_train %>% select(-Over18, -StandardHours),
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

summary(attrition_allvars) %>% 
  coef() %>% 
  tidy() %>% 
  select(`.rownames`, Estimate) %>% 
  mutate(exp_coef = exp(Estimate))
```


```{r}
confusionMatrix(data = predict(attrition_allvars, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 
# training Accuracy rate of 89% when the model fits all variables
# No information rate of 83.6%, so this model is slightly better
# sensitivity is a lot better than the previous model, at 50.9%

# CV accuracy of 87.5%
attrition_allvars$results$Accuracy
```


```{r}
# Now we made a variable importance plot to see what variables are most crucial to include in a model
vip(attrition_allvars$finalModel, num_features = 30, bar = FALSE)
```

```{r}
# Ran a logistic regression model using the top 5 best variables as shown in the Importance plot above.
set.seed(253)


attrition_bestvars <- train(
    Attrition ~ OverTime + BusinessTravel + JobInvolvement + JobSatisfaction + EnvironmentSatisfaction  ,
    data = attrition_train,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
confusionMatrix(data = predict(attrition_bestvars, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

# training Accuracy of 83.87% when fit on the top 5 best models.  Practically the same as the No information rate though...
# sensitivity is 11.2%... pretty bad...

# CV accuracy is 84.5%
attrition_bestvars$results$Accuracy
```




```{r}
# now we try a lasso model, to see if it can penalize unimportant variables and get the accuracy and sensitivity to be better
set.seed(253)

lambda_grid <- 10^seq(-4, 0, length = 100)

attrition_lasso <- train(
    Attrition ~ .,
    data = attrition_train %>% select(-Over18, -StandardHours),
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 10^seq(-4, 0, length = 100)),
    metric = "Accuracy",
    na.action = na.omit
)

attrition_lasso$results
```

#Potential best model
```{r}
#training accuracy
confusionMatrix(data = predict(attrition_lasso, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

#CV accuracy
attrition_lasso$results %>%
  filter(lambda == attrition_lasso$bestTune$lambda)

# training accuracy is 90.1% (an improvement from the NIR of 83.6%!)
# sensitivity is 50.3%

# CV accuracy is 88.1% with a lambda value of 0.0005336699	
    # (Sebastian) Maybe this is outdated? I see training accuracy = 89.3%, sensitivity = 47.9%, lambda = .001233, and CV acc = 88.2%
```



```{r}
#plot of lambda values versus accuracy
attrition_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line() +
  scale_x_log10() 


#not sure why this is here...
# attrition_lasso$resample %>% 
#   summarize(avg_accuracy = mean(Accuracy))
```
```{r}
#all vars lasso model with best lambda... but we don't rlly need this because it automatically chose the best lambda out of the range of lambdas in the model above... however, does just putting one value of lambda make it run faster? maybe we should keep this one instead..
    # (Sebastian) I think using the above chunk is fine, it doesn't take too long and I feel like it's better practice to have the best l       lambda automatically be used rather than hard coding it
attrition_lasso_best <- train(
    Attrition ~ .,
    data = attrition_train %>% select(-Over18, -StandardHours),
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 0.0005336699),
    metric = "Accuracy",
    na.action = na.omit
)

confusionMatrix(data = predict(attrition_lasso_best, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 
```
EXAMINE LASSO COEFFICIENTS TO SEE WHICH SHRUNK TO 0
```{r}
coefficients(attrition_lasso$finalModel,0.0005336699)
#DepartmentSales, EmployeeCount, JobLevel, MonthlyIncome, and PerformanceRating all shrunk to 0.


#variable importance plot on lasso model with the best lambda
vip(attrition_lasso_best$finalModel, num_features = 30, bar = FALSE)

#interesting! different variables are the most important this time around... should we try a lasso model with only the top 6-10 important variables, because the others are kinda close to 0? What do you think?
```



```{r, include=FALSE}
# Classification Tree .... but the training accuracy is literally the same as the NIR and with the "best" cp value, it sorts ALL of the cases into the "No" category for attrition, so should we get rid of this?

    # (Sebastian) probably best not to include it, I think we have plenty of plots and Lisa says specifically that we don't need to          include a bunch of plots, only the analysis that we found to be most important

set.seed(253)

attrition_tree <- train(
  Attrition ~ .,
  data = attrition_train %>% select(-Over18, -StandardHours),
  method = "rpart",
  tuneGrid = data.frame(cp = 10^seq(-4, -1 , length = 100)),
  trControl = trainControl(method = "cv", number = 5),
  metric = "Accuracy",
  na.action = na.omit
)
```


```{r, include=FALSE}
attrition_tree$results%>%
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_line()

attrition_tree$bestTune
```


```{r, include=FALSE}
confusionMatrix(data = predict(attrition_tree, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

#100 specificty??? <- because the tree always predicts no
``` 

```{r}
set.seed(253)

attrition_tree_2 <- train(
  Attrition ~ .,
  data = attrition_train %>% select(-Over18, -StandardHours),
  method = "rpart",
  tuneGrid = data.frame(cp = 0.025),
  trControl = trainControl(method = "cv", number = 5),
  metric = "Accuracy",
  na.action = na.omit
)

confusionMatrix(data = predict(attrition_tree_2, type = "raw"),
                reference = attrition_train$Attrition, 
                positive = "Yes") 

attrition_train %>%
  bind_cols(predict(attrition_tree_2, type = "prob")) %>%
  mutate(Attrition_numeric = ifelse(Attrition == "Yes",1,0)) %>%
  summarize(MSE = mean((Attrition_numeric-Yes)^2))
#average squared error between actual and predicted probabilities


attrition_train %>%
  bind_cols(predict(attrition_tree_2, type = "prob")) %>%
  ggplot(aes(x=Yes))+
  geom_histogram()
```

#MSE
